# -*- coding: utf-8 -*-
"""
æ–‡ç« ç”Ÿæˆæ¨¡å—
ä½¿ç”¨AIç”ŸæˆåŸºäºå•è¯åˆ—è¡¨çš„è‹±è¯­æ–‡ç« ï¼Œæ”¯æŒå•è¯æ ‡ç²—
"""

import re
import time
from pathlib import Path


def generate_articles_with_ai(client, model, words, count, topic, genre, difficulty, min_occurrences=2):
    """ä½¿ç”¨AIç”Ÿæˆæ–‡ç« ï¼Œæ”¯æŒå•è¯æ ‡ç²—å’Œæœ€å°å‡ºç°æ¬¡æ•°è®¾ç½®"""
    
    prompt = f"""Please write {count} English articles based on the following requirements:

Target words that MUST be included: {', '.join(words)}
Topic: {topic}
Genre: {genre}
Difficulty: {difficulty}

Requirements:
1. Across ALL {count} articles, each target word must appear at least {min_occurrences} times total
2. Write in clear, educational English
3. Make the target words BOLD using **word** format in the content
4. Ensure incorporating all target words

Output format (VERY IMPORTANT):
Please use this EXACT format for each article:

=== ARTICLE 1 ===
Title: [Article title here]
Abstract: [Brief summary in 30-50 words]
---
[Article content here with **target words** in bold]
=== END ARTICLE 1 ===

=== ARTICLE 2 ===
Title: [Article title here] 
Abstract: [Brief summary in 30-50 words]
---
[Article content here with **target words** in bold]
=== END ARTICLE 2 ===

Make sure to use **bold** formatting for all target words when they appear in the content."""

    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are an expert English teacher. Create educational articles that naturally incorporate the specified vocabulary words with bold formatting. Follow the exact format requested."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=3000,
            temperature=0.6
        )
        
        return response.choices[0].message.content
    except Exception as e:
        print(f"ç”Ÿæˆæ–‡ç« æ—¶å‡ºé”™: {e}")
        return None


def parse_and_save_articles(ai_output, word_list_name, articles_dir='articles'):
    """è§£æAIè¾“å‡ºå¹¶ä¿å­˜ä¸ºç‹¬ç«‹çš„markdownæ–‡ä»¶"""
    
    if not ai_output:
        print("æ²¡æœ‰AIè¾“å‡ºå†…å®¹å¯è§£æ")
        return []
    
    print("\n=== AIè¾“å‡ºå†…å®¹é¢„è§ˆ ===")
    preview = ai_output[:500] + "..." if len(ai_output) > 500 else ai_output
    print(preview)
    print("=" * 50)
    
    saved_articles = []
    
    # æ–¹æ³•1: ä½¿ç”¨=== ARTICLE X ===æ ¼å¼
    pattern1 = r'=== ARTICLE (\d+) ===(.*?)=== END ARTICLE \1 ==='
    matches1 = re.findall(pattern1, ai_output, re.DOTALL | re.IGNORECASE)
    
    if matches1:
        print(f"ä½¿ç”¨æ ¼å¼1è§£æåˆ° {len(matches1)} ç¯‡æ–‡ç« ")
        for num, content in matches1:
            article_info = _parse_single_article(content, num, word_list_name, articles_dir)
            if article_info:
                saved_articles.append(article_info)
    
    # å¦‚æœæ ¼å¼1å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
    if not saved_articles:
        print("æ ¼å¼1è§£æå¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•...")
        saved_articles = _fallback_parse_articles(ai_output, word_list_name, articles_dir)
    
    return saved_articles


def _parse_single_article(content, num, word_list_name, articles_dir):
    """è§£æå•ç¯‡æ–‡ç« """
    # è§£ææ ‡é¢˜å’Œæ‘˜è¦
    title_match = re.search(r'Title:\s*(.*?)(?:\n|Abstract:)', content, re.IGNORECASE)
    abstract_match = re.search(r'Abstract:\s*(.*?)(?:\n|---)', content, re.IGNORECASE | re.DOTALL)
    
    title = title_match.group(1).strip() if title_match else f"Article {num}"
    abstract = abstract_match.group(1).strip() if abstract_match else "AI generated article"
    
    # æå–æ­£æ–‡ï¼ˆ---åçš„å†…å®¹ï¼‰
    content_match = re.search(r'---\s*(.*)', content, re.DOTALL)
    article_content = content_match.group(1).strip() if content_match else content.strip()
    
    # æ¸…ç†å†…å®¹
    article_content = re.sub(r'Title:.*?\n', '', article_content, flags=re.IGNORECASE)
    article_content = re.sub(r'Abstract:.*?\n', '', article_content, flags=re.IGNORECASE)
    article_content = re.sub(r'---+', '', article_content).strip()
    
    # ä¿å­˜æ–‡ç« 
    safe_title = re.sub(r'[^\w\s-]', '', title).strip()
    safe_title = re.sub(r'[-\s]+', '-', safe_title)[:50]  # é™åˆ¶é•¿åº¦
    timestamp = time.strftime('%Y%m%d_%H%M%S')
    filename = f"{word_list_name}-{safe_title}-{timestamp}.md"
    
    markdown_content = f"""# {title}

## æ‘˜è¦
{abstract}

## æ­£æ–‡
{article_content}

---
*æœ¬æ–‡åŸºäºå•è¯åˆ—è¡¨ "{word_list_name}" ç”Ÿæˆ*
"""
    
    file_path = Path(articles_dir) / filename
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(markdown_content)
    
    print(f"å·²ä¿å­˜æ–‡ç« : {filename}")
    
    return {
        'title': title,
        'abstract': abstract,
        'filename': filename,
        'path': file_path
    }


def _fallback_parse_articles(ai_output, word_list_name, articles_dir):
    """å¤‡ç”¨è§£ææ–¹æ³•"""
    saved_articles = []
    
    # æŒ‰æ®µè½åˆ†å‰²å¹¶åˆ›å»ºæ–‡ç« 
    paragraphs = [p.strip() for p in ai_output.split('\n\n') if p.strip()]
    if len(paragraphs) >= 2:
        # å°†æ®µè½åˆ†ç»„ä¸ºæ–‡ç« 
        mid = len(paragraphs) // 2
        article_groups = [
            ('\n\n'.join(paragraphs[:mid]), "Technology and Innovation Article 1"),
            ('\n\n'.join(paragraphs[mid:]), "Technology and Innovation Article 2")
        ]
        
        for i, (content, title) in enumerate(article_groups, 1):
            safe_title = re.sub(r'[^\w\s-]', '', title).strip()
            safe_title = re.sub(r'[-\s]+', '-', safe_title)
            timestamp = time.strftime('%Y%m%d_%H%M%S')
            filename = f"{word_list_name}-{safe_title}-{timestamp}.md"
            
            markdown_content = f"""# {title}

## æ‘˜è¦
AIç”Ÿæˆçš„å…³äºæŠ€æœ¯åˆ›æ–°çš„æ–‡ç« 

## æ­£æ–‡
{content}

---
*æœ¬æ–‡åŸºäºå•è¯åˆ—è¡¨ "{word_list_name}" ç”Ÿæˆ*
"""
            
            file_path = Path(articles_dir) / filename
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
            saved_articles.append({
                'title': title,
                'abstract': "AIç”Ÿæˆçš„å…³äºæŠ€æœ¯åˆ›æ–°çš„æ–‡ç« ",
                'filename': filename,
                'path': file_path
            })
            
            print(f"å·²ä¿å­˜æ–‡ç«  {i}: {filename}")
    
    return saved_articles


def display_generated_articles(saved_articles):
    """æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ç« é¢„è§ˆ"""
    if not saved_articles:
        print("æš‚æ— å·²ç”Ÿæˆçš„æ–‡ç« ")
        return
    
    print("\n=== æ–‡ç« é¢„è§ˆ ===")
    for i, article in enumerate(saved_articles, 1):
        print(f"\nğŸ“– æ–‡ç«  {i}: {article['title']}")
        print(f"æ‘˜è¦: {article['abstract']}")
        print(f"æ–‡ä»¶: {article['filename']}")
        print("-" * 50)


def check_word_coverage(saved_articles, target_words, min_occurrences=2):
    """æ£€æŸ¥ç›®æ ‡å•è¯çš„è¦†ç›–æƒ…å†µ"""
    print("\n=== å•è¯è¦†ç›–æ£€æŸ¥ ===")
    word_counts = {word.lower(): 0 for word in target_words}
    
    for article in saved_articles:
        try:
            with open(article['path'], 'r', encoding='utf-8') as f:
                content = f.read().lower()
                for word in target_words:
                    count = content.count(word.lower())
                    word_counts[word.lower()] += count
        except Exception as e:
            print(f"æ£€æŸ¥æ–‡ç«  {article['filename']} æ—¶å‡ºé”™: {e}")
    
    print(f"ç›®æ ‡å‡ºç°æ¬¡æ•°: {min_occurrences}")
    print("å•è¯è¦†ç›–æƒ…å†µ:")
    
    insufficient_words = []
    for word, count in word_counts.items():
        status = "âœ…" if count >= min_occurrences else "âŒ"
        print(f"  {status} {word}: {count} æ¬¡")
        if count < min_occurrences:
            insufficient_words.append(word)
    
    if insufficient_words:
        print(f"\nâš ï¸ ä»¥ä¸‹å•è¯å‡ºç°æ¬¡æ•°ä¸è¶³: {insufficient_words}")
        return False
    else:
        print(f"\nğŸ‰ æ‰€æœ‰å•è¯éƒ½æ»¡è¶³æœ€å°‘å‡ºç° {min_occurrences} æ¬¡çš„è¦æ±‚ï¼")
        return True
